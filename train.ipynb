{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd9d19-e2f4-4455-a382-4e5cfde3f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers,models,optimizers,callbacks,constraints\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras.layers import Input, LSTM,GRU,TimeDistributed,Dropout, RepeatVector, Dense, Conv2D,multiply,Lambda,Add,Concatenate, Multiply,Conv2DTranspose,Layer, Reshape, ZeroPadding2D,Flatten, MaxPool3D, UpSampling2D, BatchNormalization,LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping,Callback,LearningRateScheduler,ModelCheckpoint\n",
    "from sklearn.utils import shuffle\n",
    "import math\n",
    "import os\n",
    "from tensorflow.keras import regularizers\n",
    "from scipy.linalg import qr\n",
    "tf.random.set_seed(46)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "W=20\n",
    "hdf5_file ='data/moreall_w20.hdf5'\n",
    "hdf5 = h5py.File(hdf5_file, 'r')\n",
    "N=0\n",
    "hid_dim=5\n",
    "\n",
    "number_list = list(np.arange(15))\n",
    "train_idx=list(np.arange(45))\n",
    "test_idx=[]\n",
    "\n",
    "for i in range (3):\n",
    "    w=i*15\n",
    "    test_idx.append(np.array(random.sample(number_list,3))+w)\n",
    "    \n",
    "test_idx=np.array(test_idx).reshape(-1,1)\n",
    "\n",
    "# for selected_value in test_idx:\n",
    "#     train_idx.remove(selected_value)\n",
    "# test_idx=list(test_idx.reshape(1,-1))\n",
    "\n",
    "# np.save('train_label',train_idx)\n",
    "# np.save('test_label',test_idx[0])\n",
    "\n",
    "h5k=[list(hdf5.keys())[i] for i in train_idx]\n",
    "\n",
    "for V in h5k:\n",
    "    t=len(list(hdf5[V].keys())[:-1])\n",
    "    N+=t\n",
    "    \n",
    "idx={i:[] for i in range (N)}\n",
    "\n",
    "x=0\n",
    "\n",
    "\n",
    "\n",
    "for V in h5k:\n",
    "    \n",
    "    n =list(hdf5[V].keys())[:-1]\n",
    "    for fn in n:\n",
    "        idx[x].append(V)\n",
    "        idx[x].append(fn)\n",
    "        \n",
    "#         print(idx[x])\n",
    "        x+=1\n",
    "\n",
    "index=shuffle(idx)\n",
    "# print(index)\n",
    "\n",
    "def data_generator(idx, hdf5,batch_size,if_train = True):\n",
    "    i = 0\n",
    "\n",
    "    while True:\n",
    "        X = []\n",
    "        Y = []\n",
    "        for b in range(batch_size):\n",
    "            if i == len(idx):\n",
    "                i = 0\n",
    "            alll=idx[i]\n",
    "            x = np.expand_dims(np.array(hdf5[alll[0]][alll[1]],dtype=np.float64),axis=-1)/255  # read dataset on the fly\n",
    "#             x=np.rollaxis(x,1,0)\n",
    "            x=np.nan_to_num(x)\n",
    "            \n",
    "            y_idx='trial_%04i' % int(int(alll[1][-4:])+1)\n",
    "            y = np.expand_dims(np.array(hdf5[alll[0]][y_idx],dtype=np.float64),axis=-1)/255  # read dataset on the fly\n",
    "#             y=np.rollaxis(y,1,0)\n",
    "            y=np.nan_to_num(y)\n",
    "            \n",
    "            \n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            i += 1\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        Y = np.asarray(Y)\n",
    "        \n",
    "        \n",
    "        yield [X,Y],[X,Y]\n",
    "\n",
    "        \n",
    "## input layer\n",
    "class MSE_UNSUP(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(MSE_UNSUP, self).__init__(*args, **kwargs)\n",
    "    def call(self,inputs):\n",
    "        D,A=inputs\n",
    "\n",
    "        L=tf.keras.losses.mse(D,A)\n",
    "        L=tf.reduce_mean(L)\n",
    "        self.add_loss(L*20*128*128, inputs=inputs)\n",
    "        \n",
    "        return inputs,L\n",
    "class KLDivergenceLayer(Layer):\n",
    "\n",
    "    \"\"\" Identity transform layer that adds KL divergence\n",
    "    to the final model loss.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(KLDivergenceLayer, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mu, log_var = inputs\n",
    "\n",
    "        kl_batch =  -0.5 * K.sum(1 + log_var -\n",
    "                                K.square(mu) - K.exp(log_var), axis=-1)\n",
    "\n",
    "        self.add_loss(tf.reduce_mean(kl_batch), inputs=inputs)\n",
    "\n",
    "        return inputs,tf.reduce_mean(kl_batch)\n",
    "\n",
    "input0 = Input((W,128,128, 1))\n",
    "input1 = Input((W,128,128, 1))\n",
    "\n",
    "\n",
    "def encoder():\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv2D(16, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\"),input_shape = (W,128,128, 1)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(TimeDistributed(Conv2D(16, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(TimeDistributed(Conv2D(32, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(TimeDistributed(Conv2D(32, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(TimeDistributed(Conv2D(64, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))                           \n",
    "#     model.summary()\n",
    "    return model\n",
    "encoder()\n",
    "hidden=encoder()(input0)\n",
    "encode=GRU(32)(hidden)                     \n",
    "\n",
    "z_mu = Dense(hid_dim,kernel_regularizer='l1_l2')(encode)\n",
    "z_log_var = Dense(hid_dim,kernel_regularizer='l1_l2')(encode)\n",
    "\n",
    "[A, z_log_var_A2],loss3 = KLDivergenceLayer()([z_mu, z_log_var])\n",
    "z_sigma_A = Lambda(lambda t: K.exp(.5*t))(z_log_var_A2)\n",
    "\n",
    "eps_A = K.random_normal(shape=(K.shape(input0)[0],hid_dim))\n",
    "z_eps_A2 = Multiply()([z_sigma_A, eps_A])#Multiply()([z_sigma_A, eps_A])\n",
    "z_A = Add()([A, z_eps_A2])\n",
    "\n",
    "decode = RepeatVector(W)(z_A)\n",
    "decode = GRU(32, return_sequences=True)(decode)\n",
    "decode = GRU(hidden.shape[-1], return_sequences=True)(decode)\n",
    "\n",
    "decode_future = RepeatVector(W)(z_A)\n",
    "decode_future = GRU(32, return_sequences=True)(decode_future)\n",
    "decode_future = GRU(hidden.shape[-1], return_sequences=True)(decode_future)\n",
    "\n",
    "def decoder1():\n",
    "    model1 = Sequential()\n",
    "    model1.add(TimeDistributed(Reshape((4,4,64))))\n",
    "    model1.add(TimeDistributed(Conv2DTranspose(32, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model1.add(LeakyReLU(alpha=0.05))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(TimeDistributed(Conv2DTranspose(32, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model1.add(LeakyReLU(alpha=0.05))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(TimeDistributed(Conv2DTranspose(16, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model1.add(LeakyReLU(alpha=0.05))\n",
    "    model1.add(BatchNormalization())\n",
    "\n",
    "    model1.add(TimeDistributed(Conv2DTranspose(16, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model1.add(LeakyReLU(alpha=0.05))\n",
    "    model1.add(BatchNormalization())\n",
    "\n",
    "    \n",
    "    model1.add(TimeDistributed(Conv2DTranspose(1, (5,5), strides= (2,2),activation='sigmoid',padding='same', kernel_initializer=\"he_normal\")))\n",
    "\n",
    "    return model1\n",
    "\n",
    "def decoder2():\n",
    "    model1 = Sequential()\n",
    "    model1.add(TimeDistributed(Reshape((4,4,64))))\n",
    "    model1.add(TimeDistributed(Conv2DTranspose(32, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model1.add(LeakyReLU(alpha=0.05))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(TimeDistributed(Conv2DTranspose(32, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model1.add(LeakyReLU(alpha=0.05))\n",
    "    model1.add(BatchNormalization())\n",
    "    model1.add(TimeDistributed(Conv2DTranspose(16, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model1.add(LeakyReLU(alpha=0.05))\n",
    "    model1.add(BatchNormalization())\n",
    "\n",
    "    model1.add(TimeDistributed(Conv2DTranspose(16, (5,5), strides= (2,2),padding='same', kernel_initializer=\"he_normal\")))\n",
    "    model1.add(LeakyReLU(alpha=0.05))\n",
    "    model1.add(BatchNormalization())\n",
    "\n",
    "    \n",
    "    model1.add(TimeDistributed(Conv2DTranspose(1, (5,5), strides= (2,2),activation='sigmoid',padding='same', kernel_initializer=\"he_normal\")))\n",
    "\n",
    "    return model1\n",
    "out1=decoder1()(decode)\n",
    "out2=decoder2()(decode_future)\n",
    "\n",
    "[out1,_],loss1=MSE_UNSUP()([out1,input0])\n",
    "[out2,_],loss2=MSE_UNSUP()([out2,input1])\n",
    "\n",
    "\n",
    "\n",
    "allmodel = Model(inputs=[input0,input1], outputs=[out1,out2])\n",
    "\n",
    "allmodel.add_metric(loss3, \"KL_loss\")\n",
    "allmodel.add_metric(loss2, \"future_loss\")\n",
    "\n",
    "allmodel.add_metric(loss1, \"current_loss\")\n",
    "\n",
    "allmodel.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001,clipvalue=1.0)\n",
    "allmodel.compile( optimizer=optimizer,loss='mse')\n",
    "bs=64\n",
    "\n",
    "# history1 =allmodel.fit( data_generator(index,hdf5,bs,True),validation_data=data_generator(index[-1000:],hdf5, 10,False),\n",
    "#                        batch_size=bs, epochs=100,validation_steps=np.ceil(1000/2-1),verbose=1, steps_per_epoch=np.ceil((len(idx))/300-1))\n",
    "allmodel.load_weights(\"test_model_morelatent5_20.h5\")\n",
    "# allmodel.save_weights(\"split3_latent5_20.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
